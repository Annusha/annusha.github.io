<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Anna Kukleva</title>
  
  <meta name="author" content="Anna Kukleva">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/Blue_Jays.png">
  <title>Anna Kukleva</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1'); 
</script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #2d59eb;
      text-decoration: none;
    }
    b {
      color: #000000;
      text-decoration: none;
    }
    c {
      color: #0BDA51;
      text-decoration: none;
    }
    d {
      color: #FFA500;
      text-decoration: none;
    }
    a:focus,
    a:hover {
      color: #af19bf;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
      background-image: "images/jose/bg.png";
       /*background-color: #fffff;*/
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14.5px;
      font-weight: 600
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }


    .news-scroll-box {
      height: 300px;
      overflow: auto;
      background-color: #f7f5f5;
      padding: 2px 10px 10px 10px;
    }
    
    span.highlight {
      background-color: #CA7FD4;
    }
  </style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Anna Kukleva</name>
              </p>
              <p style="text-align:justify">
              I‚Äôm  Anna Kukleva, a postdoctoral researcher at <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/">MPII</a>. I received my a PhD at <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/">Computer Vision and Machine Learning department</a> at <a href="https://www.mpi-inf.mpg.de/home/">Max Plank Institute for Informatics</a>, where I was supervised by <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/">Bernt Schiele</a> and was honored with <a href="https://www.mpi-inf.mpg.de/de/aktuelles/details/otto-hahn-medaille-fuer-anna-kukleva"> the Otto-Hahn-Medal</a> for an outstanding PhD thesis. During my PhD, my research involved close collaborations with <a href="https://hildekuehne.github.io/">Hilde Kuehne</a> (Tuebingen AI Center), and <a href="https://chrirupp.github.io/">Christian Rupprecht</a> (Oxford). I‚Äôve visited Meta FAIR and FRL labs as research intern. Particularly, in Nimble XR Input team at FRL, we were working on egocentric videos with <a href="https://scholar.google.com/citations?user=-juoweoAAAAJ&hl">Fadime Sener</a>. Prior to my PhD, I had the opportunity to work in <a href="https://www.di.ens.fr/willow/">WILLOW team</a> at Inria Paris on  multi-modal understanding of social human behaviour with <a href="https://makarandtapaswi.github.io/">Makarand Tapaswi</a> and <a href="https://www.di.ens.fr/~laptev/">Ivan Laptev</a>.  During my master, I‚Äôve been working on unsupervised video segmentation in Uni Bonn in the lab of <a href="https://pages.iai.uni-bonn.de/gall_juergen/">J√ºrgen Gall</a>.
<!--                  I‚Äôm  Anna Kukleva, a postdoctoral researcher at <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/">MPII</a> and <a href="https://tuebingen.ai/">Tubingen AI Center</a>, working with <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/">Bernt Schiele</a> and <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>. I received my a PhD at <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/">Computer Vision and Machine Learning department</a> at <a href="https://www.mpi-inf.mpg.de/home/">Max Plank Institute for Informatics</a>, where I was supervised by <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/">Bernt Schiele</a>. During my PhD, my research involved close collaborations with <a href="https://hildekuehne.github.io/">Hilde Kuehne</a> (Tuebingen AI Center), and <a href="https://chrirupp.github.io/">Christian Rupprecht</a> (Oxford). Moreover, I‚Äôve visited Meta FAIR and FRL labs as research intern. Particularly, in Nimble XR Input team at FRL, we were working on egocentric videos with <a href="https://scholar.google.com/citations?user=-juoweoAAAAJ&hl">Fadime Sener</a>. Prior to my PhD, I had the opportunity to work in <a href="https://www.di.ens.fr/willow/">WILLOW team</a> at Inria Paris on  multi-modal understanding of social human behaviour with <a href="https://makarandtapaswi.github.io/">Makarand Tapaswi</a> and <a href="https://www.di.ens.fr/~laptev/">Ivan Laptev</a>.  During my master, I‚Äôve been working on unsupervised video segmentation in Uni Bonn in the lab of <a href="https://pages.iai.uni-bonn.de/gall_juergen/">J√ºrgen Gall</a>.-->
              </p>
              <p style="text-align:justify">
              My research focuses on image and multi-modal video recognition, with a specific interest in learning representations through self-supervised, semi-supervised, and rarely fully-supervised methods.  My focus extends to exploring the transferability of these methods to few-shot and open-set generalization scenarios.
              </p>
              </p>
              <p style="text-align:center">
                <a href="mailto:akukleva@mpi-inf.mpg.de">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=eLZ_clAAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Annusha">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/annakukleva/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:0.75%;width:35%;max-width:35%">
              <a href="images/anna4.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/Anna3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--<strong style="color:red;"> Open position for hiwi project or master thesis! </strong> </em>  <br> <br>-->



        <!-- News -->
<!--        <hr>-->
        <table id="news" class="table" align="right" border="0" cellspacing="0" cellpadding="10">
        <tr class="row">
        <td class="cell">
        <a id="news"><heading>News</heading></a>
        <div class="news-scroll-box">
        <ul>

        <li>
            12/25: &nbsp &nbsp
            1 paper accepted at WACV 2026! Thanks to Siarhei and Dhimitrios!
        </li>
<!--            <br>-->
        <li>
            11/25: &nbsp &nbsp
            I'll serve as Area Chair for ICML 2026
        </li>
        <!-- ITEM -->
        <li>
               10/25: &nbsp &nbsp Visited <a href="https://vrg.fel.cvut.cz//"> Visual Recognition Group (VRG) </a> and talked about  "Unconventional LLM usage in vision tasks" at <a href="https://cmp.felk.cvut.cz/colloquium/#about"> the 50th PRCVC </a>
        </li>
          <!-- ITEM -->
        <li>
               10/25: &nbsp &nbsp <a href="https://www.nature.com/commseng/referees/reviewer-of-the-month">Reviewer of the Month</a> at Nature, Communications Engineering
      </li>
          <!-- ITEM -->
        <li>
               07/25: &nbsp &nbsp Our workshop <a href="https://what-makes-good-video.github.io/">"NextVid"</a> is accepted to NeurIPS 2025! Stay tuned!
      </li>
          <!-- ITEM -->
        <li>
               07/25: &nbsp &nbsp I'll serve as Area Chair for WACV 2026
          </li>
        <li>
               06/25: &nbsp &nbsp Honorod to receive <a href="https://www.mpg.de/prizes/otto-hahn-medal"> the Otto-Hahn-Medal</a> for my PhD thesis
          </li>
        <li>
               04/25: &nbsp &nbsp I'll serve as Area Chair for NeurIPS 2025
          </li>
         <!-- ITEM -->
        <li>
               02/25: &nbsp &nbsp 2 papers accepted at CVPR 2025! Thanks to Felix, <a href="https://phflot.github.io/">Philipp</a>, and  <a href="https://scholar.google.com/citations?user=2TTrhm0AAAAJ&hl=en">Moritz</a>!
          </li>
         <!-- ITEM -->
        <li>
               02/25: &nbsp &nbsp I'll serve as Doctoral Consortium Chair for ICCV 2025
          </li>
         <!-- ITEM -->
        <li>
               02/25: &nbsp &nbsp Our workshop <a href="https://sites.google.com/view/mmfm3rdworkshop">"What is Next in Multimodal Foundation Models?"</a> is accepted to CVPR25! Stay tuned!
          </li>
          <!-- ITEM -->
        <li>
               02/25: &nbsp &nbsp Visited <a href="https://fundamentalailab.github.io/"> FunAI group </a> and talked about  "Advancing Video Recognition with Less Supervision" </a>
          </li>
        <!-- ITEM -->
        <li>
               08/24: &nbsp &nbsp Gave Research Spotlight Talk at  <a href="https://www.gcpr-vmv.de/year/2024/program/conference-program"> GCPR 2024 </a>
          </li>
        <!-- ITEM -->
        <li>
               08/24: &nbsp &nbsp  <a href="images/phd.jpg"> Defended my PhD! üéâ üë©‚Äçüéì </a>
          </li>
        <!-- ITEM -->
        <li>
               07/24: &nbsp &nbsp <a href="https://ninatu.github.io/">Nina</a> and I talk about HowToCaption, watch here  <a href="https://www.youtube.com/watch?v=-_65pn8rcvg">Twelve Labs </a>
          </li>
        <!-- ITEM -->
        <li>
               07/24: &nbsp &nbsp Gave a talk on learning with less supervision at <a href="https://sprintml.com/">SprintML, CISPA </a>
          </li>
        <!-- ITEM -->
        <li>
               07/24: &nbsp &nbsp 1 paper accepted at ECCV 2024!
          </li>
        <!-- ITEM -->
        <li>
               06/24: &nbsp &nbsp I'll serve as Area Chair for WACV 2025
          </li>
        <!-- ITEM -->
        <li>
               06/24: &nbsp &nbsp Top reviewer at CVPR 2024
          </li>
        <!-- ITEM -->
        <li>
                03/24: &nbsp &nbsp Got accepted to  <a href="https://cvpr2023.thecvf.com/Conferences/2024/CallForDoctoralConsortium"> Doctoral Consortium </a> at CVPR24
          </li>
        <!-- ITEM -->
        <!-- ITEM -->
        <li>
                03/24: &nbsp &nbsp Gave a talk on learning with less supervision at <a href="https://www.robots.ox.ac.uk/~vgg/">VGG Reading Group, Oxford </a>
          </li>
        <!-- ITEM -->
        <li>
                03/24: &nbsp &nbsp Gave a talk on learning with less supervision at <a href="https://www.uni-siegen.de/zess/kombibox/zess-lecture-series-visual-computing.html">University of Siegen </a>
          </li>
        <!-- ITEM -->
        <li>
               02/24: &nbsp &nbsp 2 papers accepted at CVPR 2024!
          </li>
        <!-- ITEM -->
        <li>
               12/23: &nbsp &nbsp Our workshop <a href="https://sites.google.com/view/lpvl-cvpr2024">"LPVL"</a> is accepted to CVPR24! Stay tuned!
          </li>
          <!-- ITEM -->
        <li>
               11/23: &nbsp &nbsp Top reviewer at NeurIPS 2023
          </li>
          <!-- ITEM -->
        <li>
               08/23: &nbsp &nbsp Served as Area Chair for WACV 2024
          </li>
          <!-- ITEM -->
        <li>
               07/23: &nbsp &nbsp 3 papers accepted at ICCV 2023!
          </li>
          <!-- ITEM -->
        <li>
               05/23: &nbsp &nbsp Joined Meta FRL as a research intern. Working with <a href="https://scholar.google.com/citations?user=-juoweoAAAAJ&hl">Fadime Sener</a>
          </li>
          <!-- ITEM -->
          <li>
              02/23: &nbsp &nbsp Top reviewer at AISTAT 2023
          </li>

          <!-- ITEM -->
        </ul>
        </div>
        </td>
        </tr>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="news"><heading>Publications</heading></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/mm-ts.png" alt="MM-TS" width="240" height="80" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href=""> <papertitle>MM-TS: Multi-Modal Temperature and Margin Schedules for Contrastive Learning with Long-Tail Data</papertitle></a>
            <br>
            <a href="https://de.linkedin.com/in/siarhei-sheludzko-a25a0618a">Siarhei Sheludzko</a>, <a href="https://de.linkedin.com/in/dhimitriosduka">Dhimitrios Duka</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>, <b>Anna Kukleva</b>
            <br>
            <em>WACV 2026</em>

<!--        <p> <a href="">Paper</a>-->
            <br><br>
            </td>
          </tr>
        <!-- ITEM -->

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/refam.png" alt="RefAM" width="240" height="80" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2509.22650"> <papertitle>RefAM: Attention Magnets for Zero-Shot Referral Segmentation</papertitle></a>
            <br>
            <b>Anna Kukleva*</b>, <a href="https://enis.dev/">Enis Simsar*</a>,  <a href="https://alessiotonioni.github.io/">Alessio Tonioni</a>,  <a href="https://ferjad.github.io/">Muhammad Ferjad Naeem</a>,  <a href="https://federicotombari.github.io/">Federico Tombari</a>,  <a href="https://janericlenssen.github.io/">Jan Eric Lenssen</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            (*equal contribution)
            <br>
            <em>preprint, arxiv 2025</em>

        <p> <a href="https://arxiv.org/pdf/2509.22650">Paper</a> | <a href="https://github.com/Annusha/refam">Code</a>
            <br><br>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/luvit25.png" alt="LUViT" width="240" height="80" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2507.00754"> <papertitle>Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs</papertitle></a>
            <br>
            <a href="https://selimkuzucu.github.io/about/">Selim Kuzucu</a>, <a href="https://ferjad.github.io/">Muhammad Ferjad Naeem</a>,  <b>Anna Kukleva</b>, <a href="https://federicotombari.github.io/">Federico Tombari</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            <em>preprint, arxiv 2025</em>

        <p> <a href="https://arxiv.org/pdf/2507.00754">Paper</a>
            <br><br>
            </td>
          </tr>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvpr25_TFAKE.png" alt="T-FAKE" width="215" height="120" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2408.15127"> <papertitle>T-FAKE: Synthesizing Thermal Images for Facial Landmarking</papertitle></a>
            <br>
             <a href="https://phflot.github.io/">Philipp Flotho*</a>,  <a href="https://scholar.google.com/citations?user=2TTrhm0AAAAJ&hl=en">Moritz Piening*</a>, <b>Anna Kukleva</b>, <a href="https://page.math.tu-berlin.de/~steidl/">Gabriele Steidl</a>
            <br>
            (*equal contribution)
            <br>
            <em>CVPR, 2025</em>

        <p> <a href="https://arxiv.org/pdf/2408.15127">Paper</a> | <a href="https://github.com/phflot/tfake">Dataset</a>
            <br><br>
            </td>
          </tr>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvpr25_vgem.png" alt="VideoGEM" width="240" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/abs/2503.20348"> <papertitle>VideoGEM: Training-free Action Grounding in Videos</papertitle></a>
            <br>
            Felix Vogel, <a href="https://walidbousselham.com/">Walid Bousselham</a>, <b>Anna Kukleva</b>, <a href="https://ninatu.github.io/">Nina Shvetsova</a>,  <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
            <br>
            <em>CVPR, 2025</em>

        <p> <a href="https://arxiv.org/abs/2503.20348">Paper</a>    | <a href="https://github.com/felixVogel02/VideoGEM">Code</a>
            <br><br>
            </td>
          </tr>
        <!-- ITEM -->
        <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/howtocaption.png" alt="HowToCaption" width="240" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2310.04900.pdf"> <papertitle>HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</papertitle></a>
            <br>
            <a href="https://ninatu.github.io/">Nina Shvetsova*</a>, <b>Anna Kukleva*</b>, <a href="https://xudonghong.me/">Xudong Hong</a>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
            <br>
            (*equal contribution)
            <br>
            <em>ECCV, 2024</em>

        <p> <a href="https://arxiv.org/pdf/2310.04900.pdf">Paper</a> | <a href="https://github.com/ninatu/howtocaption">Code</a>
            <br><br>
            </td>
          </tr>
<!-- ITEM -->
          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/xmic.png" alt="XMIC" width="220" height="150" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href=""> <papertitle>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</papertitle></a>
            <br>
                <b>Anna Kukleva</b>, <a href="https://scholar.google.com/citations?user=-juoweoAAAAJ&hl">Fadime Sener</a>, <a href="https://scholar.google.com/citations?user=yz2P_aUAAAAJ&hl=en">Edoardo Remelli</a>, <a href="https://btekin.github.io/">Bugra Tekin</a>, <a href="https://scholar.google.com/citations?user=HDYUYvwAAAAJ&hl=en">Eric Sauser </a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://shugaoma.github.io/">Shugao Ma</a>
            <br>
            <em>CVPR, 2024</em>

        <p> <a href="https://arxiv.org/pdf/2403.19811v1.pdf">Paper</a>  | <a href="https://github.com/Annusha/xmic">Code</a>
            <br><br>
            </td>
          </tr>
          <!-- ITEM -->
          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/orco.png" alt="OrCo" width="220" height="120" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2403.18550.pdf"> <papertitle>OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning</papertitle></a>
            <br>
            Noor Ahmed*, <b>Anna Kukleva*</b>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            (*equal contribution)
            <br>
            <em>CVPR, 2024  <strong style="color:red;"> (Highlight) </strong> </em>

        <p> <a href="https://arxiv.org/pdf/2403.18550.pdf">Paper</a> | <a href="https://github.com/noorahmedds/OrCo">Code</a>
            <br><br>
            </td>
          </tr>
          <!-- ITEM -->

          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/iccv23_in_style.png" alt="InStyle" width="240" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2309.08928.pdf"> <papertitle>In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval</papertitle></a>
            <br>
            <a href="https://ninatu.github.io/">Nina Shvetsova*</a>, <b>Anna Kukleva*</b>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
            <br>
                        (*equal contribution)
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2309.08928.pdf">Paper</a> | <a href="https://github.com/ninatu/in_style">Code</a>
            <br><br>
            </td>
          </tr>



          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/iccv23_openset.png" alt="SBB" width="200" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_SSB_Simple_but_Strong_Baseline_for_Boosting_Performance_of_Open-Set_ICCV_2023_paper.pdf"> <papertitle>SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning</papertitle></a>
            <br>
            <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/yue-fan">Yue Fan</a>, <b>Anna Kukleva</b>, <a href="https://vas.mpi-inf.mpg.de/dengxin/">Dengxin Dai</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_SSB_Simple_but_Strong_Baseline_for_Boosting_Performance_of_Open-Set_ICCV_2023_paper.pdf">Paper</a> | <a href="https://github.com/YUE-FAN/SSB">Code</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/iccv23_sorting.png" alt="Sorting" width="160" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/abs/2301.02009"> <papertitle>Learning by Sorting: Self-supervised Learning with Group Ordering Constraints</papertitle></a>
            <br>
            <a href="https://ninatu.github.io/">Nina Shvetsova</a>, <a href="https://petersen.ai/">Felix Petersen</a>, <b>Anna Kukleva</b>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://arxiv.org/abs/2301.02009">Paper</a>
            <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/dagm21.png" alt="Rev" width="160" height="80" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2112.05825.pdf"> <papertitle>Revisiting Consistency Regularization for Semi-Supervised Learning</papertitle></a>
            <br>
             <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/yue-fan">Yue Fan</a>, <b>Anna Kukleva</b>,  <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            <em>IJCV, 2023</em>

                <p> <a href="https://arxiv.org/pdf/2112.05825.pdf">Paper</a>
            <br><br>
            </td>
          </tr>



          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/iclr23.jpg" alt="Temperature" width="230" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://openreview.net/pdf?id=ejHUr4nfHhD"> <papertitle>Temperature Schedules for self-supervised contrastive methods on long-tail data</papertitle></a>
            <br>
            <b>Anna Kukleva*</b>, <a href="https://moboehle.github.io/">Moritz Boehle*</a>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>
            <br>
                        (*equal contribution)
            <br>
            <em>ICLR, 2023</em>

        <p> <a href="https://openreview.net/pdf?id=ejHUr4nfHhD">Paper</a> | <a href="https://github.com/Annusha/temperature_schedules">Code</a>
            <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/eccvw2022.png" alt="Oops" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2209.11870.pdf"> <papertitle>Leveraging Self-Supervised Training for Unintentional Action Recognition</papertitle></a>
            <br>
            <a href="https://scholar.google.com/citations?user=vdRxulMAAAAJ&hl=en">Enea Duka*</a>, <b>Anna Kukleva*</b>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
                        (*equal contribution)
            <br>

            <em>ECCVW, 2022</em>

        <p> <a href="https://arxiv.org/pdf/2209.11870.pdf">Paper</a> | <a href="https://github.com/dukaenea/unintentional_actions">Code</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/eccv2022.png" alt="CycDA" width="160" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2203.16244.pdf"> <papertitle>CycDA: Unsupervised Cycle Domain Adaptation from Image to Video</papertitle></a>
            <br>
            <a href="https://wlin-at.github.io/">Wei Lin</a>, <b>Anna Kukleva</b>, <a href="https://www.researchgate.net/profile/Kunyang-Sun"> Kunyang Sun</a>, <a href="https://snototter.github.io/research/">Horst Possegger</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>, <a href="https://www.tugraz.at/institute/icg/research/team-bischof/people/team-about/horst-bischof">Horst Bischof</a>
            <br>
            <em>ECCV, 2022</em>

        <p> <a href="https://arxiv.org/pdf/2203.16244.pdf">Paper</a>
            <br><br>
            </td>
          </tr>

            <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/TAEC.png" alt="taec" width="240" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/abs/2303.05166"> <papertitle>TAEC: Unsupervised Action Segmentation with Temporal-Aware Embedding and Clustering</papertitle></a>
            <br>
             <a href="https://wlin-at.github.io/">Wei Lin</a>, <b>Anna Kukleva</b>, <a href="https://snototter.github.io/research/">Horst Possegger</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>, <a href="https://www.tugraz.at/institute/icg/research/team-bischof/people/team-about/horst-bischof">Horst Bischof</a>
            <br>
            <em>CEUR Workshop, 2023 </em>

        <p> <a href="https://arxiv.org/abs/2303.05166">Paper</a> 
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvpr22.jpg" alt="CoSSL" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CoSSL_Co-Learning_of_Representation_and_Classifier_for_Imbalanced_Semi-Supervised_Learning_CVPR_2022_paper.pdf"> <papertitle>CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning</papertitle></a>
            <br>
            <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/yue-fan">Yue Fan</a>, <a href="https://vas.mpi-inf.mpg.de/dengxin/">Dengxin Dai</a>, <b>Anna Kukleva</b>, <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>
            <br>
            <em>CVPR, 2022</em>

        <p> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CoSSL_Co-Learning_of_Representation_and_Classifier_for_Imbalanced_Semi-Supervised_Learning_CVPR_2022_paper.pdf">Paper</a> | <a href="https://github.com/YUE-FAN/CoSSL">Code</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/iccv21.png" alt="FSL" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2108.08165.pdf"> <papertitle>Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration Without Forgetting</papertitle></a>
            <br>
             <b>Anna Kukleva</b>, Hilde Kuehne,  Bernt Schiele
            <br>
            <em>ICCV, 2021</em>

        <p> <a href="https://arxiv.org/pdf/2108.08165.pdf">Paper</a> | <a href="https://github.com/Annusha/LCwoF">Code</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/wacv21.png" alt="wacv21" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://openaccess.thecvf.com/content/WACV2021/papers/VidalMata_Joint_Visual-Temporal_Embedding_for_Unsupervised_Learning_of_Actions_in_Untrimmed_WACV_2021_paper.pdf"> <papertitle>Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences</papertitle></a>
            <br>
                <a href="https://www.rvidal.me/">Rosaura G. VidalMata</a>, <a href="https://www.wjscheirer.com/">Walter J. Scheirer</a>, <b>Anna Kukleva</b>, <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/">David Cox</a>, <a href="https://hildekuehne.github.io/">Hilde Kuehne</a>
            <br>
            <em>WACV, 2021</em>

        <p> <a href="https://openaccess.thecvf.com/content/WACV2021/papers/VidalMata_Joint_Visual-Temporal_Embedding_for_Unsupervised_Learning_of_Actions_in_Untrimmed_WACV_2021_paper.pdf">Paper</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvpr20.jpg" alt="movies" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://arxiv.org/pdf/2003.13158.pd"> <papertitle>Learning Interactions and Relationships between Movie Characters</papertitle></a>
            <br>
             <b>Anna Kukleva</b>, <a href="https://makarandtapaswi.github.io/"> Makarand Tapaswi</a>,  <a href="https://www.di.ens.fr/~laptev/"> Ivan Laptev</a>
            <br>
            <em>CVPR, 2020 <strong style="color:red;"> (Oral) </strong></em>

        <p> <a href="https://arxiv.org/pdf/2003.13158.pdf">Paper</a> | <a href="https://annusha.github.io/LIReC/">Code</a>
            <br><br>
            </td>
          </tr>



          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/robotics.png" alt="robo" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="https://2019.robocup.org/downloads/program/KuklevaEtAl2019.pdf"> <papertitle>Utilizing Temporal Information in Deep Convolutional Network for Efficient Soccer Ball Detection and Tracking</papertitle></a>
            <br>
             <b>Anna Kukleva*</b>, <a href="https://mdasifkhan.github.io/"> Mohammad Asif Khan*</a>, <a href="https://www.ais.uni-bonn.de/~hfarazi/"> Hafez Farazi</a>, <a href="https://www.ais.uni-bonn.de/behnke/">Sven Behnke</a>
            <br>
                            (*equal contribution)
            <br>
            <em>RoboCup, 2019  <strong style="color:red;"> (Oral) </strong></em>

        <p> <a href="https://2019.robocup.org/downloads/program/KuklevaEtAl2019.pdf">Paper</a> | <a href="https://github.com/AIS-Bonn/TemporalBallDetection">Code</a>
            <br><br>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvpr19.png" alt="cet" width="240" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <br>
                 <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf"> <papertitle>Unsupervised Learning of Action Classes with Continuous Temporal Embedding</papertitle></a>
            <br>
             <b>Anna Kukleva*</b>, <a href="https://hildekuehne.github.io/">Hilde Kuehne*</a>,  <a href="https://scholar.google.com/citations?user=-juoweoAAAAJ&hl">Fadime Sener</a>, <a href="https://pages.iai.uni-bonn.de/gall_juergen/">J√ºrgen Gall</a>
            <br>
                (*equal contribution)
            <br>
            <em>CVPR, 2019 </em>

        <p> <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.pdf">Paper</a> | <a href="https://github.com/Annusha/unsup_temp_embed">Code</a>
            </td>
          </tr>



        </tbody></table>

        
        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Talks</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <ul>
                <li>
                    10/2025: Visited <a href="https://vrg.fel.cvut.cz//"> Visual Recognition Group (VRG) </a> and talked about "Unconventional LLM usage in vision tasks" at <a href="https://cmp.felk.cvut.cz/colloquium/#about"> the 50th Pattern Recognition and Computer Vision Colloquium </a>, Czech Technical University, Prague
                </li>
                <li>
                    02/2025: Talk on advancing video recognition with less supervision at  <a href="https://fundamentalailab.github.io/"> FunAI group, UTN </a>
                </li>
                <li>
                    09/2024: Research Spotlight Talk at  <a href="https://www.gcpr-vmv.de/year/2024/program/conference-program"> GCPR 2024 </a>
                </li>
                <li>
                    08/2024: "HowToCaption: Prompting LLMs to Transform Video Annotations at Scale" at The 57th session of MultimodalWeekly  <a href="https://www.youtube.com/watch?v=-_65pn8rcvg">Twelve Labs </a>
                </li>
                <li>
                    07/2024: Talk on learning with less supervision at <a href="https://sprintml.com/">SprintML, CISPA </a>
                </li>
                <li>
                    03/2024: Talk on learning with less supervision at <a href="https://www.robots.ox.ac.uk/~vgg/">VGG Reading Group, Oxford </a>
                </li>
                <li>
                    03/2024: Talk on learning with less supervision at <a href="https://www.uni-siegen.de/zess/kombibox/zess-lecture-series-visual-computing.html">University of Siegen</a>
                </li>
               <li>
                   02/2024: Moderate Q&A session of amazing talk by <a href="https://yxw.web.illinois.edu/">Yuxiong Wang </a> who talks about  <a href="https://www.youtube.com/watch?v=9c9KdJ-5JG4"> "Bridging Generative & Discriminative Learning in the Open Worlds"</a>, Virtual
                </li>
                <li>
                  01/2022: Talk at <a href="https://www.youtube.com/watch?v=i6ZbnnKIACI">Compuer Vision Talks</a>, Virtual
                </li>
                <li>
                  12/2020: Talk at <a href="https://www.youtube.com/watch?v=TixJu8p9QZQ"> 6th Christmas Colloquium on Computer Vision 2020</a>, Samsung AI Center Moscow, Virtual
                </li>
              <li>
                  08/2020: Talk at <a href="https://www.notion.so/dlcv/Welcome-to-the-Deep-Learning-Computer-Vision-Practitioners-community-1acb5d6d9e6f461f84f6f0d88e90d608">DLCV Practioner‚Äôs Evening</a>, Virtual
                </li>
              <li>
                  01/2020: Talk at <a href="http://www.cs.cmu.edu/~abhinavg/"> Abhinav Gupta‚Äôs</a> group at CMU in Pittsburgh, USA
                </li>
              <li>
                  11/2019: Talk at <a href="https://www.di.ens.fr/willow/events/workshop26nov2019/"> WILLOW-ENPC-Berkeley Workshop on Vision and Robotics</a> in Paris, France
                </li>
              <li>
                  09/2019: Poster at <a href="http://people.cs.bris.ac.uk/~damen/bmva_symposium_2019/"> BMVA symposium on Video Understanding</a> in London, UK
                </li>
              <li>
                  06/2019: Poster at <a href="https://wicvworkshop.github.io/CVPR2019/index.html"> WiCV workshop</a> in conjunction with CVPR 2019 in Long Beach, USA
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>


        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Academic</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <ul>
                <li>
                  Doctoral Consortium Chair: ICCV 2025
                </li>
                <li>
                  Area Chair: WACV24-26, NeurIPS25, ICML26
                </li>
                <li>
                    Top Reviewer: AISTAT23, NeurIPS23, CVPR24
                </li>
                <li>
                  Reviewer: CVPR21-26 (w/o 25), ICCV21-23, ECCV22-24, AISTAT23, NeurIPS23-24, ICML25, WACV21-22, PAMI, IJCV, TMM, ACMMM21, Nature Commseng
                </li>
                <li>
                    10/2025: Reviewer of the Month: <a href="https://www.nature.com/commseng/referees/reviewer-of-the-month">Nature CommsEng</a>
                </li>
                <li>
                  06/2025: <a href="https://www.mpg.de/prizes/otto-hahn-medal"> Otto-Hahn-Medal</a> (PhD Thesis Award), Max-Planck-Gesellschaft
                </li>
              <li>
                  07/2021: <a href="https://www.informatik.uni-bonn.de/de/gleichstellung/talents"> Grace-Hopper-Award</a> (Master Thesis Award), Uni Bonn
                </li>
              <li>
                  ECCV 2020: Coorganizing the <a href="https://sites.google.com/view/wicvworkshop-eccv2020/home"> WiCV workshop</a>, Virtual
                </li>
              <li>
                  CVPR 2024: Coorganizing workshop <a href="https://sites.google.com/view/lpvl-cvpr2024">"Learning from Procedural Videos and Language: What is Next?"</a>, Seattle
                </li>
                <li>
                  CVPR 2025: Coorganizing workshop <a href="https://sites.google.com/view/mmfm3rdworkshop">"MMFM3: The 3rd Workshop on What is Next in Multimodal Foundation Models?"</a>, San Diego
                </li>
                <li>
                  NeurIPS 2025: Coorganizing workshop <a href="https://what-makes-good-video.github.io/">"(NextVid) What Makes a Good Video: Next Practices in Video Generation and Evaluation"</a> , Nashville
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>

				

      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Stolen from <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks! 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
